{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"EPO full-text for humans \u00b6 Explore the EP full-text data for text analytics (CC4) in a minute. Quick Start : overview of the dataset Developers : standard ETL pipeline to fit the dataset into an easy-to-use/query format This is an open-source project ( MIT-2 ). EP full-text data for text analytics The European Patent Office (EPO) has released a bulk data collection. It contains the full text of all patent applications/grants since it was set up in 1978. EPO official data documentation EPO official user guide EPO authority file EPO offical data on Google Cloud Storage","title":"About"},{"location":"#epo-full-text-for-humans","text":"Explore the EP full-text data for text analytics (CC4) in a minute. Quick Start : overview of the dataset Developers : standard ETL pipeline to fit the dataset into an easy-to-use/query format This is an open-source project ( MIT-2 ). EP full-text data for text analytics The European Patent Office (EPO) has released a bulk data collection. It contains the full text of all patent applications/grants since it was set up in 1978. EPO official data documentation EPO official user guide EPO authority file EPO offical data on Google Cloud Storage","title":"EPO full-text for humans"},{"location":"before/","text":"Before starting \u00b6 Compress data \u00b6 Both EPO .txt files and serailized .jsonl files are uncompressed . Hence, you can save a significant amount of space by compressing these files. Snippet gzip your/folder/EP*.* Warning Although bq load can handle compressed files, it cannot read this type of files asynchronously . This means that loading the full database from compressed EP*.jsonl.gz files will last much longer (around 15 minutes). In short, you had better make sure that the database is properly loaded before compressing the EP*.jsonl files. Check data \u00b6 Validate schema \u00b6 You can validate the json schema of each patent in the .jsonl files using the validate-schema.py CLI. If there is an error, it will send a Warning with the file name, the line number and the json object which raised the error. Validate schema python bin/validate-schema.py \"your/folder/EP*.jsonl\" Number of lines by EP0*.jsonl files \u00b6 You can count the number of lines in the .jsonl objects to make sure that the serialization job did not fail silently. Expected results here . Count lines for file in $( find you/folder/EP*.jsonl ) ; do > echo \" $file \" ; > wc -l \" $file \" ; > done Duplicates \u00b6 There are 954 publication_number duplicates. Manual inspection shows that they differ by their publication dates. Duplicates Number of duplicates WITH tmp AS ( SELECT publication_number , COUNT ( publication_number ) AS count_ FROM ` project . dataset . epo_fulltext ` GROUP BY publication_number ) SELECT COUNT ( DISTINCT ( publication_number )) AS nb_pubnum_duplicates FROM tmp WHERE count_ > 1 Insights on duplicates WITH tmp AS ( SELECT publication_number , COUNT ( publication_number ) AS count_ FROM ` project . dataset . epo_fulltext ` GROUP BY publication_number ) SELECT * FROM tmp WHERE count_ > 1 ) SELECT dup . publication_number , epo . publication_date , epo . abstract , epo . description , epo . title FROM dup , ` npl - parsing . external . epo_fulltext ` AS epo WHERE dup . publication_number = epo . publication_number ORDER BY publication_number","title":"Before starting"},{"location":"before/#before-starting","text":"","title":"Before starting"},{"location":"before/#compress-data","text":"Both EPO .txt files and serailized .jsonl files are uncompressed . Hence, you can save a significant amount of space by compressing these files. Snippet gzip your/folder/EP*.* Warning Although bq load can handle compressed files, it cannot read this type of files asynchronously . This means that loading the full database from compressed EP*.jsonl.gz files will last much longer (around 15 minutes). In short, you had better make sure that the database is properly loaded before compressing the EP*.jsonl files.","title":"Compress data"},{"location":"before/#check-data","text":"","title":"Check data"},{"location":"before/#validate-schema","text":"You can validate the json schema of each patent in the .jsonl files using the validate-schema.py CLI. If there is an error, it will send a Warning with the file name, the line number and the json object which raised the error. Validate schema python bin/validate-schema.py \"your/folder/EP*.jsonl\"","title":"Validate schema"},{"location":"before/#number-of-lines-by-ep0jsonl-files","text":"You can count the number of lines in the .jsonl objects to make sure that the serialization job did not fail silently. Expected results here . Count lines for file in $( find you/folder/EP*.jsonl ) ; do > echo \" $file \" ; > wc -l \" $file \" ; > done","title":"Number of lines by EP0*.jsonl files"},{"location":"before/#duplicates","text":"There are 954 publication_number duplicates. Manual inspection shows that they differ by their publication dates. Duplicates Number of duplicates WITH tmp AS ( SELECT publication_number , COUNT ( publication_number ) AS count_ FROM ` project . dataset . epo_fulltext ` GROUP BY publication_number ) SELECT COUNT ( DISTINCT ( publication_number )) AS nb_pubnum_duplicates FROM tmp WHERE count_ > 1 Insights on duplicates WITH tmp AS ( SELECT publication_number , COUNT ( publication_number ) AS count_ FROM ` project . dataset . epo_fulltext ` GROUP BY publication_number ) SELECT * FROM tmp WHERE count_ > 1 ) SELECT dup . publication_number , epo . publication_date , epo . abstract , epo . description , epo . title FROM dup , ` npl - parsing . external . epo_fulltext ` AS epo WHERE dup . publication_number = epo . publication_number ORDER BY publication_number","title":"Duplicates"},{"location":"extract/","text":"Extract data \u00b6 Preview with the Google Storage User Interface (optional) \u00b6 Navigate to the epo-patentinformation bucket to have a preview of the dataset. Warning The total size of the bulk data exceeds 200Gb. Do not even think about downloading the full dataset using the UI. It will fail. Download the dataset with gsutil \u00b6 To download the EPO bulk dataset using the console: Install gsutil , the google cloud Command Line Interface (CLI) to interact with Google Storage. Quickstart and Installation guide . Download the dataset to your/destination/folder gsutil -u <your-billing-project> \\ # specify the billing project -m cp -r gs://epo-patentinformation/ \\ <your/destination/folder> Tip If you are a frequent user of the Google Cloud Platform, you can set your/destination/folder to a Google Storage bucket uri (e.g. gs://... ). The rest of the pipeline can be executed from a compute instance with the bucket mounted , see gcsfuse instructions.","title":"Extract"},{"location":"extract/#extract-data","text":"","title":"Extract data"},{"location":"extract/#preview-with-the-google-storage-user-interface-optional","text":"Navigate to the epo-patentinformation bucket to have a preview of the dataset. Warning The total size of the bulk data exceeds 200Gb. Do not even think about downloading the full dataset using the UI. It will fail.","title":"Preview with the Google Storage User Interface (optional)"},{"location":"extract/#download-the-dataset-with-gsutil","text":"To download the EPO bulk dataset using the console: Install gsutil , the google cloud Command Line Interface (CLI) to interact with Google Storage. Quickstart and Installation guide . Download the dataset to your/destination/folder gsutil -u <your-billing-project> \\ # specify the billing project -m cp -r gs://epo-patentinformation/ \\ <your/destination/folder> Tip If you are a frequent user of the Google Cloud Platform, you can set your/destination/folder to a Google Storage bucket uri (e.g. gs://... ). The rest of the pipeline can be executed from a compute instance with the bucket mounted , see gcsfuse instructions.","title":"Download the dataset with gsutil"},{"location":"license/","text":"License \u00b6 Copyright 2020 Cyril VERLUISE Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"License"},{"location":"license/#license","text":"Copyright 2020 Cyril VERLUISE Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"License"},{"location":"load/","text":"Load \u00b6 BigQuery offers a convenient way to query and analyze large amounts of data. Info In case you are new to BigQuery, you might want to: Take the Google BigQuery Quickstart Learn more on bq load Data schema \u00b6 To load a table on BigQuery, you need to specify its schema. CreateSchema.py (python CLI) generates this schema for you. Take care to set the --prepare-names / --no-prepare-names option to the value set when you serialized the data. python bin/create-schema.py \\ --prepare-names \\ path/to/schema.json # destination file Load table \u00b6 Tip For the sake of efficiency, load the serialized files to a Google Storage bucket beforehand gsutil -m cp -r path/to/folder/ gs://your-bucket/ bq load --source_format = NEWLINE_DELIMITED_JSON \\ --ignore_unknown_values \\ --replace \\ --max_bad_records = 10 \\ project:dataset.table \\ path/to/EP*.jsonl \\ # gs://your-bucket/EP*.jsonl recommended path/to/schema.json","title":"Load"},{"location":"load/#load","text":"BigQuery offers a convenient way to query and analyze large amounts of data. Info In case you are new to BigQuery, you might want to: Take the Google BigQuery Quickstart Learn more on bq load","title":"Load"},{"location":"load/#data-schema","text":"To load a table on BigQuery, you need to specify its schema. CreateSchema.py (python CLI) generates this schema for you. Take care to set the --prepare-names / --no-prepare-names option to the value set when you serialized the data. python bin/create-schema.py \\ --prepare-names \\ path/to/schema.json # destination file","title":"Data schema"},{"location":"load/#load-table","text":"Tip For the sake of efficiency, load the serialized files to a Google Storage bucket beforehand gsutil -m cp -r path/to/folder/ gs://your-bucket/ bq load --source_format = NEWLINE_DELIMITED_JSON \\ --ignore_unknown_values \\ --replace \\ --max_bad_records = 10 \\ project:dataset.table \\ path/to/EP*.jsonl \\ # gs://your-bucket/EP*.jsonl recommended path/to/schema.json","title":"Load table"},{"location":"quickstart/","text":"Quick Start \u00b6 Figures and analyses reported below can be reproduced using the material in the exploratory data analysis folder. EP data (1978-2019) \u00b6 Number of EPO patents and patent families \u00b6 Patent family A patent family is a collection of patent applications covering the same or similar technical content. The applications in a family are related to each other through priority claims. From now on, we work at the family level. Each family is assigned to the year of it first appearance in the EP full-text patent dataset. This date might differ from its priority year. EP full-text data (1978-2019) \u00b6 Number and share of EPO patent families with full-text data \u00b6 Number Share title abstract description claims amendment url 1978-2019 0.6 0.2 0.33 0.33 0.0 0.41 Overall, 1/3 of EP families have at least one patent with a full-text description. Number and share of EPO patents and patent families with full-text data in english \u00b6 Number Share en_title en_abstract en_description en_claims en_amendment en_url 1978-2019 0.22 0.02 0.22 0.22 0.0 0.22 Overall, 1/5 of EP families have at least one patent with a full-text description in english . Worldwide coverage of full-text patent data \u00b6 Leverage patent families to enlarge full-text data coverage Patents in the same (simple) family usually have the same text components. This means that any patent text can be propagated to all patents in the same family. Thanks to that, the EP (resp US) full text data can cover patents well beyond the the EP (resp US) office. We use this tip to look at the implicit worldwide coverage of the EP and US full-text patents. In this section we focus on the availability of the full-text patent descriptions . USPTO and EPO full-text patent description data coverage \u00b6 USPTO full text data The USPTO full text data are available from the USPTO bulk data . US full-text patent data can be easily parsed thanks to iamlemec/patents . They are also available in a well-structured format as part of the BigQuery patents-public-dataset . USPTO EPO Paradoxically, we observe that the USPTO full-text data implicitly covers a larger share of EP families than the EP bulk dataset. The best of both worlds \u00b6 We can also pool the families covered by the EPO and the USPTO full-text bulk datasets. This is the largest coverage of full-text patent data that can be obtained from publicly available datasets.","title":"Quick start"},{"location":"quickstart/#quick-start","text":"Figures and analyses reported below can be reproduced using the material in the exploratory data analysis folder.","title":"Quick Start"},{"location":"quickstart/#ep-data-1978-2019","text":"","title":"EP data (1978-2019)"},{"location":"quickstart/#number-of-epo-patents-and-patent-families","text":"Patent family A patent family is a collection of patent applications covering the same or similar technical content. The applications in a family are related to each other through priority claims. From now on, we work at the family level. Each family is assigned to the year of it first appearance in the EP full-text patent dataset. This date might differ from its priority year.","title":"Number of EPO patents and patent families"},{"location":"quickstart/#ep-full-text-data-1978-2019","text":"","title":"EP full-text data (1978-2019)"},{"location":"quickstart/#number-and-share-of-epo-patent-families-with-full-text-data","text":"Number Share title abstract description claims amendment url 1978-2019 0.6 0.2 0.33 0.33 0.0 0.41 Overall, 1/3 of EP families have at least one patent with a full-text description.","title":"Number and share of EPO patent families with full-text data"},{"location":"quickstart/#number-and-share-of-epo-patents-and-patent-families-with-full-text-data-in-english","text":"Number Share en_title en_abstract en_description en_claims en_amendment en_url 1978-2019 0.22 0.02 0.22 0.22 0.0 0.22 Overall, 1/5 of EP families have at least one patent with a full-text description in english .","title":"Number and share of EPO patents and patent families with full-text data in english"},{"location":"quickstart/#worldwide-coverage-of-full-text-patent-data","text":"Leverage patent families to enlarge full-text data coverage Patents in the same (simple) family usually have the same text components. This means that any patent text can be propagated to all patents in the same family. Thanks to that, the EP (resp US) full text data can cover patents well beyond the the EP (resp US) office. We use this tip to look at the implicit worldwide coverage of the EP and US full-text patents. In this section we focus on the availability of the full-text patent descriptions .","title":"Worldwide coverage of full-text patent data"},{"location":"quickstart/#uspto-and-epo-full-text-patent-description-data-coverage","text":"USPTO full text data The USPTO full text data are available from the USPTO bulk data . US full-text patent data can be easily parsed thanks to iamlemec/patents . They are also available in a well-structured format as part of the BigQuery patents-public-dataset . USPTO EPO Paradoxically, we observe that the USPTO full-text data implicitly covers a larger share of EP families than the EP bulk dataset.","title":"USPTO and EPO full-text patent description data coverage"},{"location":"quickstart/#the-best-of-both-worlds","text":"We can also pool the families covered by the EPO and the USPTO full-text bulk datasets. This is the largest coverage of full-text patent data that can be obtained from publicly available datasets.","title":"The best of both worlds"},{"location":"set-up/","text":"Set up \u00b6 Google Cloud Platform \u00b6 To get access to the data you need: A Google Cloud Platform (GCP) account A GCP project with enabled billing Info In case you are new to GCP and want to learn the basics of Google Storage (the storage service of GCP), you can take the Google Storage Quickstart . This should not take more than 2 minutes and might help a lot ! Parse EPO \u00b6 Clone the repository git clone https://github.com/cverluise/parseEPO.git Install requirements poetry cd parseEPO/ poetry install requirements.txt # create and activate virtual environment first cd parseEPO/ pip install -r requirements.txt poetry - Recommended! Poetry is a tool for dependency management and packaging in Python. It guarantees that all dependencies and sub-dependencies are exactly the same as those of the initial project. It also manages the virtual environment for you.","title":"Set-up"},{"location":"set-up/#set-up","text":"","title":"Set up"},{"location":"set-up/#google-cloud-platform","text":"To get access to the data you need: A Google Cloud Platform (GCP) account A GCP project with enabled billing Info In case you are new to GCP and want to learn the basics of Google Storage (the storage service of GCP), you can take the Google Storage Quickstart . This should not take more than 2 minutes and might help a lot !","title":"Google Cloud Platform"},{"location":"set-up/#parse-epo","text":"Clone the repository git clone https://github.com/cverluise/parseEPO.git Install requirements poetry cd parseEPO/ poetry install requirements.txt # create and activate virtual environment first cd parseEPO/ pip install -r requirements.txt poetry - Recommended! Poetry is a tool for dependency management and packaging in Python. It guarantees that all dependencies and sub-dependencies are exactly the same as those of the initial project. It also manages the virtual environment for you.","title":"Parse EPO"},{"location":"transform/","text":"Serialize data \u00b6 EPO full-text tsv data limitations \u00b6 The EP full text data are served in a tab-separated value (tsv) format. E.g. EP 0600083 A1 1994-06-08 de TITLE VORRICHTUNG ZUM F\u00d6RDERN UND ORIENTIEREN VON PAPIERBOGEN EP 0600083 A1 1994-06-08 en TITLE DEVICE FOR CONVEYING AND ARRANGING PAPER SHEET EP 0600083 A1 1994-06-08 fr TITLE DISPOSITIF D'ACHEMINEMENT ET DE POSITIONNEMENT DE FEUILLES DE PAPIER EP 0600083 A1 1994-06-08 en ABSTR <p id=\"pa01\" num=\"0001\">A device for conveying ... EP 0600083 A1 1994-06-08 en DESCR <heading id=\"h0001\">Field of Technology</heading><p id=\"p0001\" num=\"0001\">This ... EP 0600083 A1 1994-06-08 en CLAIM <claim id=\"c-en-0001\" num=\"0001\"><claim-text>A paper sheet conveying ... EP 0600083 A1 1994-06-08 en PDFEP https://data.epo.org/publication-server/pdf-document?cc=EP&pn=0600083&ki=A1&pd=1994-06-08 ... This comes with two drawbacks: Redundant information (e.g. EP 0600083 A1 ) Not supported by major big data tools (e.g. BigQuery) ParseEPO : EPO full-text for humans \u00b6 We overcome these drawbacks by serializing the data. Each patent is represented by a single json object with nested fields. E.g. { \"publication_number\" : \"EP-0600083-A1\" , \"publication_date\" : \"1994-06-08\" , \"country_code\" :[ \"de\" , \"en\" , \"fr\" ], \"title\" : { \"language\" :[ \"de\" , \"en\" , \"fr\" ] , \"text\" :[ \"VORRICHTUNG ZUM F\u00d6RDERN UND ORIENTIEREN VON PAPIERBOGEN\\n\" , \"DEVICE FOR CONVEYING AND ARRANGING PAPER SHEET\\n\" , \"DISPOSITIF D'ACHEMINEMENT ET DE POSITIONNEMENT DE FEUILLES DE PAPIER\\n\" ] } , \"abstract\" : { \"text\" : \"<p id=\\\"pa01\\\" num=\\\"0001\\\">A device for ...\" , \"language\" : \"en\" } , \"claims\" : { \"language\" :[ \"en\" ] , \"text\" :[ \"<claim id=\\\"c-en-0001\\\" num=\\\"0001\\\"><claim-text>A paper ...\" ] } , \"description\" : { \"text\" : \"<heading id=\\\"h0001\\\">Field of Technology</heading><p ...\" , \"language\" : \"en\" } , \"url\" : { \"text\" : \"https://data.epo.org/publication-server/pdf-document?cc=EP&pn=0600083&ki=A1&pd=1994-06-08\\n\" , \"language\" : \"en\" } , } Note In this example, variable names have been slightly modified (e.g. ABSTR becomes abstract ). This is meant to align variable names with BigQuery patents data standards. You can avoid this behavior by setting --no-prepare-names , see tip:SerializeEPO.py below. In practice \u00b6 SerializeEPO.py (python CLI) turns the EP tsv files into json newline delimited files. python bin/serialize-epo.py \\ --max-workers 2 \\ --verbose \\ --prepare-names \\ \"your/folder/EP*.txt\" SerializeEPO.py Each file is serialized and the output is saved in your/folder/ as <epo-file-name>.jsonl( .<suffix>) . Nb: if the original file was compressed ( .gz ), the serialized file will be compressed as well. --max-workers : Maximum number of threads allowed --verbose / --no-verbose : Display info on-going process --prepare-names / --no-prepare-names : Prepare names in line with BigQuery patents data standards --handle-html / --no-handle-html : Handle html --help : Show this message and exit.","title":"Transform"},{"location":"transform/#serialize-data","text":"","title":"Serialize data"},{"location":"transform/#epo-full-text-tsv-data-limitations","text":"The EP full text data are served in a tab-separated value (tsv) format. E.g. EP 0600083 A1 1994-06-08 de TITLE VORRICHTUNG ZUM F\u00d6RDERN UND ORIENTIEREN VON PAPIERBOGEN EP 0600083 A1 1994-06-08 en TITLE DEVICE FOR CONVEYING AND ARRANGING PAPER SHEET EP 0600083 A1 1994-06-08 fr TITLE DISPOSITIF D'ACHEMINEMENT ET DE POSITIONNEMENT DE FEUILLES DE PAPIER EP 0600083 A1 1994-06-08 en ABSTR <p id=\"pa01\" num=\"0001\">A device for conveying ... EP 0600083 A1 1994-06-08 en DESCR <heading id=\"h0001\">Field of Technology</heading><p id=\"p0001\" num=\"0001\">This ... EP 0600083 A1 1994-06-08 en CLAIM <claim id=\"c-en-0001\" num=\"0001\"><claim-text>A paper sheet conveying ... EP 0600083 A1 1994-06-08 en PDFEP https://data.epo.org/publication-server/pdf-document?cc=EP&pn=0600083&ki=A1&pd=1994-06-08 ... This comes with two drawbacks: Redundant information (e.g. EP 0600083 A1 ) Not supported by major big data tools (e.g. BigQuery)","title":"EPO full-text tsv data limitations"},{"location":"transform/#parseepo-epo-full-text-for-humans","text":"We overcome these drawbacks by serializing the data. Each patent is represented by a single json object with nested fields. E.g. { \"publication_number\" : \"EP-0600083-A1\" , \"publication_date\" : \"1994-06-08\" , \"country_code\" :[ \"de\" , \"en\" , \"fr\" ], \"title\" : { \"language\" :[ \"de\" , \"en\" , \"fr\" ] , \"text\" :[ \"VORRICHTUNG ZUM F\u00d6RDERN UND ORIENTIEREN VON PAPIERBOGEN\\n\" , \"DEVICE FOR CONVEYING AND ARRANGING PAPER SHEET\\n\" , \"DISPOSITIF D'ACHEMINEMENT ET DE POSITIONNEMENT DE FEUILLES DE PAPIER\\n\" ] } , \"abstract\" : { \"text\" : \"<p id=\\\"pa01\\\" num=\\\"0001\\\">A device for ...\" , \"language\" : \"en\" } , \"claims\" : { \"language\" :[ \"en\" ] , \"text\" :[ \"<claim id=\\\"c-en-0001\\\" num=\\\"0001\\\"><claim-text>A paper ...\" ] } , \"description\" : { \"text\" : \"<heading id=\\\"h0001\\\">Field of Technology</heading><p ...\" , \"language\" : \"en\" } , \"url\" : { \"text\" : \"https://data.epo.org/publication-server/pdf-document?cc=EP&pn=0600083&ki=A1&pd=1994-06-08\\n\" , \"language\" : \"en\" } , } Note In this example, variable names have been slightly modified (e.g. ABSTR becomes abstract ). This is meant to align variable names with BigQuery patents data standards. You can avoid this behavior by setting --no-prepare-names , see tip:SerializeEPO.py below.","title":"ParseEPO: EPO full-text for humans"},{"location":"transform/#in-practice","text":"SerializeEPO.py (python CLI) turns the EP tsv files into json newline delimited files. python bin/serialize-epo.py \\ --max-workers 2 \\ --verbose \\ --prepare-names \\ \"your/folder/EP*.txt\" SerializeEPO.py Each file is serialized and the output is saved in your/folder/ as <epo-file-name>.jsonl( .<suffix>) . Nb: if the original file was compressed ( .gz ), the serialized file will be compressed as well. --max-workers : Maximum number of threads allowed --verbose / --no-verbose : Display info on-going process --prepare-names / --no-prepare-names : Prepare names in line with BigQuery patents data standards --handle-html / --no-handle-html : Handle html --help : Show this message and exit.","title":"In practice"}]}