{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"EPO full-text for humans \u00b6 Explore the EP full-text data for text analytics (CC4) in a minute. \ud83d\udea7 WiP \ud83d\udea7: We have prepared the data for you. You can start exploring the full database using BigQuery right now. Efficient full-scale analysis is literally one click away from you. Developers : This project is based on a standard ETL pipeline. Advanced users can build on it to design their own implementation and/or set options that are most convenient to them. This is an open-source project ( MIT-2 ). EP full-text data for text analytics The European Patent Office (EPO) has released a bulk data collection. It contains the full text of all patent applications/grants since it was set up in 1978. EPO official data documentation EPO offical data on Google Cloud Storage","title":"About"},{"location":"#epo-full-text-for-humans","text":"Explore the EP full-text data for text analytics (CC4) in a minute. \ud83d\udea7 WiP \ud83d\udea7: We have prepared the data for you. You can start exploring the full database using BigQuery right now. Efficient full-scale analysis is literally one click away from you. Developers : This project is based on a standard ETL pipeline. Advanced users can build on it to design their own implementation and/or set options that are most convenient to them. This is an open-source project ( MIT-2 ). EP full-text data for text analytics The European Patent Office (EPO) has released a bulk data collection. It contains the full text of all patent applications/grants since it was set up in 1978. EPO official data documentation EPO offical data on Google Cloud Storage","title":"EPO full-text for humans"},{"location":"extract/","text":"Extract data \u00b6 Preview with the Google Storage User Interface (optional) \u00b6 Navigate to the epo-patentinformation bucket to have a preview of the dataset. Warning The total size of the bulk data exceeds 200Gb. Do not even think about downloading the full dataset using the UI. It will fail. Download the dataset with gsutil \u00b6 To download the EPO bulk dataset using the console: Install gsutil , the google cloud Command Line Interface (CLI) to interact with Google Storage. Quickstart and Installation guide . Download the dataset to your/destination/folder gsutil -u <your-billing-project> \\ -m cp -r gs://epo-patentinformation/ \\ <your/destination/folder> Tip The EPO dataset is made of uncompressed .txt files. Hence, you can divide its size 5-folds by compressing .txt files in .gz files. Execute gzip your/destination/folder/EP*.txt . Note: the rest of the pipeline supports .gz files natively. If you are a frequent user of the Google Cloud Platform, you can set your/destination/folder to a Google Storage bucket. The rest of the pipeline can be executed from a compute instance with the bucket mounted , see gcsfuse instructions.","title":"Extract"},{"location":"extract/#extract-data","text":"","title":"Extract data"},{"location":"extract/#preview-with-the-google-storage-user-interface-optional","text":"Navigate to the epo-patentinformation bucket to have a preview of the dataset. Warning The total size of the bulk data exceeds 200Gb. Do not even think about downloading the full dataset using the UI. It will fail.","title":"Preview with the Google Storage User Interface (optional)"},{"location":"extract/#download-the-dataset-with-gsutil","text":"To download the EPO bulk dataset using the console: Install gsutil , the google cloud Command Line Interface (CLI) to interact with Google Storage. Quickstart and Installation guide . Download the dataset to your/destination/folder gsutil -u <your-billing-project> \\ -m cp -r gs://epo-patentinformation/ \\ <your/destination/folder> Tip The EPO dataset is made of uncompressed .txt files. Hence, you can divide its size 5-folds by compressing .txt files in .gz files. Execute gzip your/destination/folder/EP*.txt . Note: the rest of the pipeline supports .gz files natively. If you are a frequent user of the Google Cloud Platform, you can set your/destination/folder to a Google Storage bucket. The rest of the pipeline can be executed from a compute instance with the bucket mounted , see gcsfuse instructions.","title":"Download the dataset with gsutil"},{"location":"license/","text":"License \u00b6 Copyright 2020 Cyril VERLUISE Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"License"},{"location":"license/#license","text":"Copyright 2020 Cyril VERLUISE Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"License"},{"location":"load/","text":"Load \u00b6 BigQuery offers a convenient way to query and analyze large amounts of data. Info In case you are new to BigQuery, you might want to: Take the Google BigQuery Quickstart Learn more on bq load Data schema \u00b6 To load a table on BigQuery, you need to specify its schema. CreateSchema.py (python CLI) generates this schema for you. Take care to set the --prepare-names / --no-prepare-names option to the value set when you serialized the data. python bin/create-schema.py \\ --prepare-names \\ path/to/schema.json # destination file Tip create-schema.py currently misses rare variables (e.g. AMEND ). If you want to generate the full schema, you can still use the generate-schema CLI. E.g. generate-schema < path/to/EP0600000.jsonl > path/to/schema.json . Load table \u00b6 Tip For the sake of efficiency, load the serialized files to a Google Storage bucket beforehand ( gsutil -m cp -r path/to/folder/ gs://your-bucket/ ) bq load --source_format = NEWLINE_DELIMITED_JSON \\ --ignore_unknown_values \\ --replace \\ --max_bad_records = 10 \\ project:dataset.table \\ path/to/EP*.jsonl \\ # path/to/EP*.jsonl.gz if compressed beforehand # gs://your-bucket/EP*.jsonl(.gz) recommended path/to/schema.json","title":"Load"},{"location":"load/#load","text":"BigQuery offers a convenient way to query and analyze large amounts of data. Info In case you are new to BigQuery, you might want to: Take the Google BigQuery Quickstart Learn more on bq load","title":"Load"},{"location":"load/#data-schema","text":"To load a table on BigQuery, you need to specify its schema. CreateSchema.py (python CLI) generates this schema for you. Take care to set the --prepare-names / --no-prepare-names option to the value set when you serialized the data. python bin/create-schema.py \\ --prepare-names \\ path/to/schema.json # destination file Tip create-schema.py currently misses rare variables (e.g. AMEND ). If you want to generate the full schema, you can still use the generate-schema CLI. E.g. generate-schema < path/to/EP0600000.jsonl > path/to/schema.json .","title":"Data schema"},{"location":"load/#load-table","text":"Tip For the sake of efficiency, load the serialized files to a Google Storage bucket beforehand ( gsutil -m cp -r path/to/folder/ gs://your-bucket/ ) bq load --source_format = NEWLINE_DELIMITED_JSON \\ --ignore_unknown_values \\ --replace \\ --max_bad_records = 10 \\ project:dataset.table \\ path/to/EP*.jsonl \\ # path/to/EP*.jsonl.gz if compressed beforehand # gs://your-bucket/EP*.jsonl(.gz) recommended path/to/schema.json","title":"Load table"},{"location":"set-up/","text":"Set up \u00b6 Google Cloud Platform \u00b6 To get access to the data you need: A Google Cloud Platform (GCP) account A GCP project with enabled billing Info In case you are new to GCP and want to learn the basics of Google Storage (the storage service of GCP), you can take the Google Storage Quickstart . This should not take more than 2 minutes and might help a lot ! Parse EPO \u00b6 Clone the repository git clone https://github.com/cverluise/parseEPO.git Install requirements poetry cd parseEPO/ poetry install requirements.txt # create and activate virtual environment first cd parseEPO/ pip install -r requirements.txt poetry - Recommended! Poetry is a tool for dependency management and packaging in Python. It guarantees that all dependencies and sub-dependencies are exactly the same as those of the initial project. It also manages the virtual environment for you.","title":"Set-up"},{"location":"set-up/#set-up","text":"","title":"Set up"},{"location":"set-up/#google-cloud-platform","text":"To get access to the data you need: A Google Cloud Platform (GCP) account A GCP project with enabled billing Info In case you are new to GCP and want to learn the basics of Google Storage (the storage service of GCP), you can take the Google Storage Quickstart . This should not take more than 2 minutes and might help a lot !","title":"Google Cloud Platform"},{"location":"set-up/#parse-epo","text":"Clone the repository git clone https://github.com/cverluise/parseEPO.git Install requirements poetry cd parseEPO/ poetry install requirements.txt # create and activate virtual environment first cd parseEPO/ pip install -r requirements.txt poetry - Recommended! Poetry is a tool for dependency management and packaging in Python. It guarantees that all dependencies and sub-dependencies are exactly the same as those of the initial project. It also manages the virtual environment for you.","title":"Parse EPO"},{"location":"transform/","text":"Serialize data \u00b6 EPO full-text tsv data limitations \u00b6 The EP full text data are served in a tab-separated value (tsv) format. E.g. EP 0600083 A1 1994-06-08 de TITLE VORRICHTUNG ZUM F\u00d6RDERN UND ORIENTIEREN VON PAPIERBOGEN EP 0600083 A1 1994-06-08 en TITLE DEVICE FOR CONVEYING AND ARRANGING PAPER SHEET EP 0600083 A1 1994-06-08 fr TITLE DISPOSITIF D'ACHEMINEMENT ET DE POSITIONNEMENT DE FEUILLES DE PAPIER EP 0600083 A1 1994-06-08 en ABSTR <p id=\"pa01\" num=\"0001\">A device for conveying ... EP 0600083 A1 1994-06-08 en DESCR <heading id=\"h0001\">Field of Technology</heading><p id=\"p0001\" num=\"0001\">This ... EP 0600083 A1 1994-06-08 en CLAIM <claim id=\"c-en-0001\" num=\"0001\"><claim-text>A paper sheet conveying ... EP 0600083 A1 1994-06-08 en PDFEP https://data.epo.org/publication-server/pdf-document?cc=EP&pn=0600083&ki=A1&pd=1994-06-08 ... This comes with two drawbacks: Redundant information (e.g. EP 0600083 A1 ) Not supported by major big data tools (e.g. BigQuery) ParseEPO : EPO full-text for humans \u00b6 We overcome these drawbacks by serializing the data. Each patent is represented by a single json object with nested fields. E.g. { \"publication_number\" : \"EP-0600083-A1\" , \"publication_date\" : \"1994-06-08\" , \"country_code\" :[ \"de\" , \"en\" , \"fr\" ], \"title\" : { \"language\" :[ \"de\" , \"en\" , \"fr\" ] , \"text\" :[ \"VORRICHTUNG ZUM F\u00d6RDERN UND ORIENTIEREN VON PAPIERBOGEN\\n\" , \"DEVICE FOR CONVEYING AND ARRANGING PAPER SHEET\\n\" , \"DISPOSITIF D'ACHEMINEMENT ET DE POSITIONNEMENT DE FEUILLES DE PAPIER\\n\" ] } , \"abstract\" : { \"text\" : \"<p id=\\\"pa01\\\" num=\\\"0001\\\">A device for ...\" , \"language\" : \"en\" } , \"claims\" : { \"language\" :[ \"en\" ] , \"text\" :[ \"<claim id=\\\"c-en-0001\\\" num=\\\"0001\\\"><claim-text>A paper ...\" ] } , \"description\" : { \"text\" : \"<heading id=\\\"h0001\\\">Field of Technology</heading><p ...\" , \"language\" : \"en\" } , \"url\" : { \"text\" : \"https://data.epo.org/publication-server/pdf-document?cc=EP&pn=0600083&ki=A1&pd=1994-06-08\\n\" , \"language\" : \"en\" } , } Note In this example, variable names have been slightly modified (e.g. ABSTR becomes abstract ). This is meant to align variable names with BigQuery patents data standards. You can avoid this behavior by setting --no-prepare-names (see tip:SerializeEPO.py below.) In practice \u00b6 SerializeEPO.py (python CLI) turns the EP tsv files into json newline delimited files. python bin/serialize-epo.py \\ --max-workers 2 \\ --verbose \\ --prepare-names \\ \"your/folder/EP*.txt\" # \"your/folder/EP*.txt.gz\" if compressed beforehand SerializeEPO.py Each file is serialized and the output is saved in your/folder/ as <epo-file-name>.jsonl( .<suffix>) . Nb: if the original file was compressed ( .gz ), the serialized file will be compressed as well. --max-workers : Maximum number of threads allowed --verbose / --no-verbose : Display info on-going process --prepare-names / --no-prepare-names : Prepare names in line with BigQuery patents data standards --handle-html / --no-handle-html : Handle html --help : Show this message and exit.","title":"Transform"},{"location":"transform/#serialize-data","text":"","title":"Serialize data"},{"location":"transform/#epo-full-text-tsv-data-limitations","text":"The EP full text data are served in a tab-separated value (tsv) format. E.g. EP 0600083 A1 1994-06-08 de TITLE VORRICHTUNG ZUM F\u00d6RDERN UND ORIENTIEREN VON PAPIERBOGEN EP 0600083 A1 1994-06-08 en TITLE DEVICE FOR CONVEYING AND ARRANGING PAPER SHEET EP 0600083 A1 1994-06-08 fr TITLE DISPOSITIF D'ACHEMINEMENT ET DE POSITIONNEMENT DE FEUILLES DE PAPIER EP 0600083 A1 1994-06-08 en ABSTR <p id=\"pa01\" num=\"0001\">A device for conveying ... EP 0600083 A1 1994-06-08 en DESCR <heading id=\"h0001\">Field of Technology</heading><p id=\"p0001\" num=\"0001\">This ... EP 0600083 A1 1994-06-08 en CLAIM <claim id=\"c-en-0001\" num=\"0001\"><claim-text>A paper sheet conveying ... EP 0600083 A1 1994-06-08 en PDFEP https://data.epo.org/publication-server/pdf-document?cc=EP&pn=0600083&ki=A1&pd=1994-06-08 ... This comes with two drawbacks: Redundant information (e.g. EP 0600083 A1 ) Not supported by major big data tools (e.g. BigQuery)","title":"EPO full-text tsv data limitations"},{"location":"transform/#parseepo-epo-full-text-for-humans","text":"We overcome these drawbacks by serializing the data. Each patent is represented by a single json object with nested fields. E.g. { \"publication_number\" : \"EP-0600083-A1\" , \"publication_date\" : \"1994-06-08\" , \"country_code\" :[ \"de\" , \"en\" , \"fr\" ], \"title\" : { \"language\" :[ \"de\" , \"en\" , \"fr\" ] , \"text\" :[ \"VORRICHTUNG ZUM F\u00d6RDERN UND ORIENTIEREN VON PAPIERBOGEN\\n\" , \"DEVICE FOR CONVEYING AND ARRANGING PAPER SHEET\\n\" , \"DISPOSITIF D'ACHEMINEMENT ET DE POSITIONNEMENT DE FEUILLES DE PAPIER\\n\" ] } , \"abstract\" : { \"text\" : \"<p id=\\\"pa01\\\" num=\\\"0001\\\">A device for ...\" , \"language\" : \"en\" } , \"claims\" : { \"language\" :[ \"en\" ] , \"text\" :[ \"<claim id=\\\"c-en-0001\\\" num=\\\"0001\\\"><claim-text>A paper ...\" ] } , \"description\" : { \"text\" : \"<heading id=\\\"h0001\\\">Field of Technology</heading><p ...\" , \"language\" : \"en\" } , \"url\" : { \"text\" : \"https://data.epo.org/publication-server/pdf-document?cc=EP&pn=0600083&ki=A1&pd=1994-06-08\\n\" , \"language\" : \"en\" } , } Note In this example, variable names have been slightly modified (e.g. ABSTR becomes abstract ). This is meant to align variable names with BigQuery patents data standards. You can avoid this behavior by setting --no-prepare-names (see tip:SerializeEPO.py below.)","title":"ParseEPO: EPO full-text for humans"},{"location":"transform/#in-practice","text":"SerializeEPO.py (python CLI) turns the EP tsv files into json newline delimited files. python bin/serialize-epo.py \\ --max-workers 2 \\ --verbose \\ --prepare-names \\ \"your/folder/EP*.txt\" # \"your/folder/EP*.txt.gz\" if compressed beforehand SerializeEPO.py Each file is serialized and the output is saved in your/folder/ as <epo-file-name>.jsonl( .<suffix>) . Nb: if the original file was compressed ( .gz ), the serialized file will be compressed as well. --max-workers : Maximum number of threads allowed --verbose / --no-verbose : Display info on-going process --prepare-names / --no-prepare-names : Prepare names in line with BigQuery patents data standards --handle-html / --no-handle-html : Handle html --help : Show this message and exit.","title":"In practice"}]}